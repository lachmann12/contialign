{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import archs4py as a4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ENST00000631435'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg = \">ENST00000631435.1 cdna chromosome:GRCh38:CHR_HSCHR7_2_CTG6:142847306:142847317:1 gene:ENSG00000282253.1 gene_biotype:TR_D_gene transcript_biotype:TR_D_gene gene_symbol:TRBD1 description:T cell receptor beta diversity 1 [Source:HGNC Symbol;Acc:HGNC:12158]\"\n",
    "gg.split(\" \")[0].split(\".\")[0].replace(\">\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205541\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(\"data\")\n",
    "\n",
    "def parse_transcripts(file_path):\n",
    "    sequences = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            if lines[i].startswith('>'):\n",
    "                #transcript_id = re.search(r'>(\\S+)', lines[i]).group(1)\n",
    "                #transcript_id = re.sub(r'\\.\\d+', '', transcript_id)\n",
    "                transcript_id = lines[i].split(\" \")[0].split(\".\")[0].replace(\">\", \"\")\n",
    "                sequence = ''\n",
    "                i += 1\n",
    "                while i < len(lines) and not lines[i].startswith('>'):\n",
    "                    sequence += lines[i].strip()\n",
    "                    i += 1\n",
    "                sequences[transcript_id] = sequence\n",
    "    return sequences\n",
    "\n",
    "file_path = '../tempest/files/Homo_sapiens.GRCh38.cdna.all.fa'  # Replace with the actual file path\n",
    "#file_path = 'Homo_sapiens.GRCh38.cdna.all.fa'  # Replace with the actual file path\n",
    "transcripts = parse_transcripts(file_path)\n",
    "print(len(list(transcripts.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271742/271742 [01:39<00:00, 2732.34it/s] \n"
     ]
    }
   ],
   "source": [
    "def extract_subsequence(transcripts, transcript_id, length):\n",
    "    if transcript_id in transcripts:\n",
    "        sequence = transcripts[transcript_id]\n",
    "        seq_length = len(sequence)\n",
    "        if len(sequence) >= length:\n",
    "            start_position = random.randint(0, seq_length - length)\n",
    "            subsequence = sequence[start_position:start_position + length]\n",
    "            return subsequence\n",
    "        else:\n",
    "            #print(f\"Transcript {transcript_id} is shorter than the requested length.\")\n",
    "            return \"\"\n",
    "    else:\n",
    "        #print(f\"Transcript {transcript_id} not found.\")\n",
    "        #print(transcript_id)\n",
    "        return \" \"\n",
    "\n",
    "def mutate_sequence(sequence, mutation_probability):\n",
    "    mutated_sequence = ''\n",
    "    for nucleotide in sequence:\n",
    "        if random.random() < mutation_probability:\n",
    "            mutated_nucleotide = random.choice('ACTG'.replace(nucleotide, ''))\n",
    "            mutated_sequence += mutated_nucleotide\n",
    "        else:\n",
    "            mutated_sequence += nucleotide\n",
    "    return mutated_sequence\n",
    "\n",
    "files = os.listdir(\"data\")\n",
    "files = [f for f in os.listdir(\"data\") if f != '.DS_Store']\n",
    "counts = pd.read_csv(\"data/\"+files[0], sep=\"\\t\")\n",
    "\n",
    "reads = []\n",
    "errors = 0\n",
    "not_found = 0\n",
    "for i in tqdm.tqdm(range(counts.shape[0])):\n",
    "    t = counts.iloc[i, 0]\n",
    "    count = counts.iloc[i, 1]\n",
    "    for c in range(count):\n",
    "        read = extract_subsequence(transcripts, t, 100)\n",
    "        if len(read) > 1:\n",
    "            read = mutate_sequence(read, 0.00)\n",
    "            reads.append(read)\n",
    "        elif len(read) == 1:\n",
    "            not_found += 1\n",
    "        else:\n",
    "            errors += 1\n",
    "\n",
    "with open(\"syndata.fastq\", \"w\") as f:\n",
    "    counter = 0\n",
    "    qual = 'J'*100\n",
    "    for r in reads:\n",
    "        counter+=1\n",
    "        f.write(f\"@syn.{counter} xx length=100\\n\")\n",
    "        f.write(f\"{r}\\n\")\n",
    "        f.write(f\"+syn.{counter} xx length=100\\n\")\n",
    "        f.write(f\"{qual}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no such file or directory: target/release/tempest\n"
     ]
    }
   ],
   "source": [
    "!\"target/release/tempest\" -f \"../synthetic/syndata.fastq\" -t 6 -o \"output/counts_syn.tsv\" -x \"output/syn.idx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11964787\n",
      "11964787\n"
     ]
    }
   ],
   "source": [
    "print(len(reads))\n",
    "print(counts.iloc[:,1].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m big_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(\u001b[39m800000\u001b[39m, \u001b[39m1280\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m index \u001b[39m=\u001b[39m faiss\u001b[39m.\u001b[39mindex_factory(\u001b[39m1280\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mIVF2048,Flat\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m index\u001b[39m.\u001b[39;49mtrain(big_data)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Divide the dataset into smaller parts\u001b[39;00m\n\u001b[1;32m     11\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m1000000\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/faiss/class_wrappers.py:298\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_train\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39massert\u001b[39;00m d \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md\n\u001b[1;32m    297\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mascontiguousarray(x, dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 298\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_c(n, swig_ptr(x))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/faiss/swigfaiss.py:5359\u001b[0m, in \u001b[0;36mIndexIVF.train\u001b[0;34m(self, n, x)\u001b[0m\n\u001b[1;32m   5357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, n, x):\n\u001b[1;32m   5358\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\" Trains the quantizer and calls train_residual to train sub-quantizers\"\"\"\u001b[39;00m\n\u001b[0;32m-> 5359\u001b[0m     \u001b[39mreturn\u001b[39;00m _swigfaiss\u001b[39m.\u001b[39;49mIndexIVF_train(\u001b[39mself\u001b[39;49m, n, x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have n vectors of dimension d in a numpy array\n",
    "big_data = np.random.rand(800000, 1280).astype('float32')\n",
    "\n",
    "index = faiss.index_factory(1280, \"IVF2048,Flat\")\n",
    "index.train(big_data)\n",
    "\n",
    "# Divide the dataset into smaller parts\n",
    "batch_size = 1000000\n",
    "start_idx = 0\n",
    "batches = []\n",
    "while start_idx < len(big_data):\n",
    "    end_idx = min(start_idx + batch_size, len(big_data))\n",
    "    batches.append(big_data[start_idx:end_idx])\n",
    "    start_idx = end_idx\n",
    "\n",
    "def search(query_vec, k):\n",
    "    top_distance = []\n",
    "    top_indexes = []\n",
    "\n",
    "    for batch in batches:\n",
    "        index.add(batch)\n",
    "        distance, indexes = index.search(query_vec, k)\n",
    "        index.reset()\n",
    "\n",
    "        if top_indexes:\n",
    "            merged_distance = np.hstack((top_distance, distance))\n",
    "            merged_indexes = np.hstack((top_indexes, indexes))\n",
    "            new_sort_indices = np.argsort(merged_distance)[:, :k]\n",
    "            top_distance = np.array([merged_distance[i, new_sort_indices[i]] for i in range(merged_distance.shape[0])])\n",
    "            top_indexes = np.array([merged_indexes[i, new_sort_indices[i]] for i in range(merged_indexes.shape[0])])\n",
    "        else:\n",
    "            top_distance = distance\n",
    "            top_indexes = indexes\n",
    "\n",
    "    return top_distance, top_indexes\n",
    "\n",
    "query_vectors = np.random.rand(10, 1280).astype('float32')  # Generate 10 random query vectors\n",
    "k = 10  # Number of nearest neighbors to find\n",
    "\n",
    "distance,indexes = search(query_vectors, k)\n",
    "\n",
    "print(\"Distances:\\n\", distance)\n",
    "print(\"Indexes:\\n\", indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "big_data = np.random.rand(80000, 1280).astype('float32')\n",
    "\n",
    "coarse_quantizer = faiss.IndexFlatL2(1280)\n",
    "index = faiss.IndexIVFFlat(coarse_quantizer, 1280, 1024)\n",
    "index.train(big_data)\n",
    "\n",
    "# Divide the dataset into smaller parts and save them on disk\n",
    "batch_size = 10000\n",
    "start_idx = 0\n",
    "part = 0\n",
    "while start_idx < len(big_data):\n",
    "    end_idx = min(start_idx + batch_size, len(big_data))\n",
    "    batch = big_data[start_idx:end_idx]\n",
    "    \n",
    "    file_name = f'batch_{part}.npy'\n",
    "    np.save(file_name, batch)\n",
    "    start_idx = end_idx\n",
    "    part += 1\n",
    "\n",
    "index.make_direct_map()\n",
    "faiss.write_index(index, \"batch.idx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = faiss.read_index(\"batch.idx\")\n",
    "part = 0\n",
    "file_name = f'batch_{part}.npy'\n",
    "batch = np.load(file_name)\n",
    "index.add(batch)\n",
    "\n",
    "query_vectors = np.random.rand(2, 1280).astype('float32')  # Generate 10 random query vectors\n",
    "k = 5  # Number of nearest neighbors to find\n",
    "distance, indexes = index.search(query_vectors, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[196.58798 198.8354  200.25943 200.42883 201.83769]\n",
      " [204.06442 207.50412 207.85512 208.13489 208.60117]]\n",
      "[[6971 5974 1396 7973 1482]\n",
      " [2929 8690 9727 2723 1653]]\n"
     ]
    }
   ],
   "source": [
    "print(distance)\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24608397483825684\n",
      "Distances:\n",
      " [[  0.      190.92966 194.85794 195.5722  195.98907]\n",
      " [  0.      187.3742  188.67325 188.97668 189.07774]]\n",
      "Indexes:\n",
      " [[   0 1943 6802  541 3230]\n",
      " [   1 9048 5361  243  326]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "def search(index, query_vec, k, prefix):\n",
    "\n",
    "    files = glob.glob(os.path.join(\"\", prefix + \"_*.npy\"))\n",
    "    \n",
    "    top_distance = []\n",
    "    top_indexes = []\n",
    "\n",
    "    for file_name in files:\n",
    "        batch = np.load(file_name)\n",
    "\n",
    "        index.add(batch)\n",
    "        distance, indexes = index.search(query_vec, k)\n",
    "        index.reset()\n",
    "        \n",
    "        if len(top_distance) == 0:\n",
    "            top_distance = distance\n",
    "            top_indexes = indexes\n",
    "        else:\n",
    "            merged_distance = np.hstack((top_distance, distance))\n",
    "            merged_indexes = np.hstack((top_indexes, indexes))\n",
    "            new_sort_indices = np.argsort(merged_distance)[:, :k]\n",
    "            top_distance = np.array([merged_distance[i, new_sort_indices[i]] for i in range(merged_distance.shape[0])])\n",
    "            top_indexes = np.array([merged_indexes[i, new_sort_indices[i]] for i in range(merged_indexes.shape[0])])\n",
    "\n",
    "    return top_distance, top_indexes\n",
    "\n",
    "index = faiss.read_index(\"batch.idx\")\n",
    "#query_vectors = np.random.rand(2, 1280).astype('float32')  # Generate 10 random query vectors\n",
    "query_vectors = big_data[0:2,:]\n",
    "k = 5  # Number of nearest neighbors to find\n",
    "\n",
    "st = time.time()\n",
    "distance, indexes = search(index, query_vectors, k, \"batch\")\n",
    "print(time.time()-st)\n",
    "\n",
    "print(\"Distances:\\n\", distance)\n",
    "print(\"Indexes:\\n\", indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1280)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_data[0:2,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1280)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
